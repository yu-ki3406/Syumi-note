{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained MDP (CMDP)\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はCMDPを考えたアルゴリズムの中でもOpt_CMDPというアルゴリズムを考えます．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.33333333]\n",
      " [ 0.         -0.66666667]\n",
      " [ 0.         -1.        ]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "[[[1. 0. 0.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[0. 1. 0.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 1.]\n",
      "  [1. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 環境の定義\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "states = np.arange(0,3)\n",
    "actions = {0:[0,1],1:[0,1],2:[0,1]} # 0:遷移しない, 1:遷移する\n",
    "\n",
    "S = len(states)\n",
    "A = 2\n",
    "\n",
    "#遷移確率と報酬とコストを定義\n",
    "P  = np.zeros((S, A, S))\n",
    "R = np.zeros((S, A))\n",
    "C = np.zeros((S, A))\n",
    "\n",
    "for s in range(S):\n",
    "    for a in range(A):\n",
    "\n",
    "        if a == 0:\n",
    "            P[s,a,s] = 1\n",
    "        else:\n",
    "            R[s,a] = -(s + 1)\n",
    "            C[s,a] = 1\n",
    "\n",
    "            if s < S-1:\n",
    "                P[s,a,s+1] = 1\n",
    "            else:\n",
    "                P[s,a,0] = 1\n",
    "             \n",
    "#報酬の絶対値の大きさを1以下にする\n",
    "for s in range(S):\n",
    "    for a in range(A):\n",
    "        R[s][a] = R[s][a]/3\n",
    "\n",
    "print(R)\n",
    "print(C)\n",
    "print(P)\n",
    "\n",
    "states_abs = np.arange(0,1)\n",
    "nA_abs = 2\n",
    "\n",
    "\n",
    "N_STATES_abs = len(states_abs)\n",
    "\n",
    "P_abs = np.zeros((1,2,1))\n",
    "R_abs = np.zeros((1,2))\n",
    "C_abs = np.zeros((1,2))\n",
    "\n",
    "P_abs[0][0][0] = 1\n",
    "P_abs[0][1][0] = 1\n",
    "\n",
    "R_abs[0][1] = 2\n",
    "\n",
    "C_abs[0][1] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_LENGTH = 6\n",
    "CONSTRAINT = EPISODE_LENGTH/2\n",
    "C_b = CONSTRAINT/5\n",
    "NUMBER_EPISODES = 1e5\n",
    "NUMBER_SIMULATIONS = 1\n",
    "\n",
    "eps = 0.01\n",
    "M = 0\n",
    "delta = 0.01\n",
    "\n",
    "P_hat = np.zeros((S, A, S))\n",
    "R_hat = np.zeros((S, A))\n",
    "C_hat = np.zeros((S, A))\n",
    "Total_emp_reward = np.zeros((S, A))\n",
    "Total_emp_cost = np.zeros((S ,A))\n",
    "\n",
    "s_a_visit = np.zeros((S, A))\n",
    "s_a_s_visit = np.zeros((S, A, S))\n",
    "beta_prob = np.zeros((S,A,S))\n",
    "beta_prob_2 = np.zeros((S,A,S))\n",
    "beta_r = np.zeros((S,A))\n",
    "beta_prob_T = np.zeros((S,A,S))\n",
    "\n",
    "mu = np.zeros((S))\n",
    "mu[0] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#次の状態や報酬を返す関数   \n",
    "def step(state,action):\n",
    "    next_state = int(np.random.choice(np.arange(S),1,replace=True,p=P[state,action,:]))\n",
    "    rew = R[state,action]\n",
    "    cost = C[state,action]\n",
    "    return next_state,rew,cost\n",
    "\n",
    "#訪問回数を更新する関数\n",
    "def setCounts(s_a_s_count,s_a_count):\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            s_a_visit[s,a] += s_a_count[s,a]\n",
    "            for s_ in range(S):\n",
    "                s_a_s_visit[s,a,s_] += s_a_s_count[s,a,s_]\n",
    "\n",
    "#経験的な遷移関数の更新\n",
    "def update_P_hat():\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "\n",
    "            if s_a_visit[s][a] == 0:\n",
    "                P_hat[s][a] = 1 / S * np.ones(S)\n",
    "            \n",
    "            else:\n",
    "                for s_ in range(S):\n",
    "                    P_hat[s][a][s_] = s_a_s_visit[s][a][s_] / s_a_visit[s][a]\n",
    "                \n",
    "                P_hat[s][a] = P_hat[s][a] / np.sum(P_hat[s][a])\n",
    "\n",
    "#経験的な報酬とコストの更新\n",
    "def update_RC_hat(emp_r,emp_c):\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            R_hat[s][a] = (Total_emp_reward[s][a] + emp_r[s][a]) / (max(s_a_visit[s][a],1))\n",
    "            C_hat[s][a] = (Total_emp_cost[s][a] + emp_c[s][a]) / (max(s_a_visit[s][a],1))\n",
    "\n",
    "            Total_emp_reward += emp_r\n",
    "            Total_emp_cost += emp_c\n",
    "\n",
    "#信頼区間の計算\n",
    "def calc_confidence_interval():\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            beta_r[s][a] = np.sqrt(1/max(1,s_a_visit[s][a]))\n",
    "            for s_ in range(S):\n",
    "                beta_prob[s][a][s_] = np.sqrt(P_hat[s][a][s_]*(1-P_hat[s][a][s_])/max(s_a_visit[s][a],1)) + 1/(max(s_a_visit[s][a],1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 価値関数やコスト関数の計算\n",
    "def Policy_evaluation(P,policy,R,C):\n",
    "    Q_h = np.zeros((S,EPISODE_LENGTH,A))\n",
    "    V_h = np.zeros((S,EPISODE_LENGTH))\n",
    "    C_h = np.zeros((S,EPISODE_LENGTH))\n",
    "\n",
    "    P_policy = np.zeros((S,EPISODE_LENGTH,S))\n",
    "    R_policy = np.zeros((S,EPISODE_LENGTH))\n",
    "    C_policy = np.zeros((S,EPISODE_LENGTH))\n",
    "\n",
    "\n",
    "    \n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            C_h[s][EPISODE_LENGTH-1] += policy[s][EPISODE_LENGTH-1][a] * C[s][a]\n",
    "            Q_h[s,EPISODE_LENGTH-1,a] = R[s][a]\n",
    "\n",
    "        V_h[s,EPISODE_LENGTH-1] = np.dot(Q_h[s,EPISODE_LENGTH-1,:],policy[s,EPISODE_LENGTH-1,:])\n",
    "    \n",
    "    for h in range(EPISODE_LENGTH):\n",
    "        for s in range(S):\n",
    "            for a in range(A):\n",
    "                C_policy[s][h] += policy[s][h][a] * C[s][a]\n",
    "            \n",
    "            for s_ in range(S):\n",
    "                for a in range(A):\n",
    "                    P_policy[s,h,s_] += policy[s][h][a] * P[s][a][s_]\n",
    "    \n",
    "    for h in range(EPISODE_LENGTH,-1,-1): #0~4\n",
    "        for s in range(S):\n",
    "            C_h[s,h] = C_policy[s,h] + np.dot(P_policy[s,h,:],C_h[:,h+1])\n",
    "            for a in range(A):\n",
    "                P_V = 0\n",
    "                for s_ in range(S):\n",
    "                    P_v += P_policy[s,h,s_] * V_h[s_,h+1]\n",
    "                Q_h[s,h,a] = R[s][a] + P_V\n",
    "            V_h[s,h] = np.dot(Q_h[s,h,:],policy[s,h,:])\n",
    "    \n",
    "    return Q_h,V_h,C_h\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "                \n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ここからExtended LP問題を考えるのですが，この[サイト](https://qiita.com/ytakashina/items/9de38882ffa611d5a07a)からpulpの大きいモデルではnumpy.sumを避ける理由が書いてあります．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.33333333, 0.33333333])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CMDPを解くための線形計画法\n",
    "import pulp\n",
    "import math\n",
    "\n",
    "def solve_CMDP():\n",
    "    policy = np.zeros((S,EPISODE_LENGTH,A))\n",
    "    problem = pulp.LpProblem('CMDP', pulp.LpMinimize)\n",
    "    z_opt = np.zeros((EPISODE_LENGTH,S,A,S))\n",
    "    z_keys = [(h,s,a,s_) for h in range(EPISODE_LENGTH) for s in range(S) for a in range(A) for s_ in range(S)]\n",
    "    z = pulp.LpVariable.dicts('z_var',z_keys,0,1,cat='Continuous')\n",
    "    \n",
    "    #最適したい問題\n",
    "    problem += pulp.lpSum([z[(h,s,a,s_)] * R[s][a] for h in range(EPISODE_LENGTH) for s in range(S) for a in range(A) for s_ in range(S)])\n",
    "\n",
    "    #制約条件\n",
    "    problem += pulp.lpSum([z[(h,s,a,s_)] * C[s][a] for h in range(EPISODE_LENGTH) for s in range(S) for a in range(A) for s_ in range(S)]) <= CONSTRAINT\n",
    "\n",
    "    for h in range(1,EPISODE_LENGTH):\n",
    "        for s in range(S):\n",
    "            z_list = [z[(h,s,a,s_)] for a in range(A) for s_ in range(S)]\n",
    "            z_prev_list = [z[(h-1,s_,a,s)] for a in range(A) for s_ in range(S)]\n",
    "            problem += pulp.lpSum(z_list) == pulp.lpSum(z_prev_list)\n",
    "\n",
    "    for s in range(S):\n",
    "        z_1_lsit = [z[(0,s,a,s_)] for a in range(A) for s_ in range(S)]\n",
    "        problem += pulp.lpSum(z_1_lsit) == mu[s]\n",
    "    \n",
    "    problem += [z[(h,s,a,s_)] for h in range(EPISODE_LENGTH) for s in range(S) for a in range(A) for s_ in range(S)] >= 0\n",
    "\n",
    "    for h in range(EPISODE_LENGTH):\n",
    "            for s in range(S):\n",
    "                for a in range(A):\n",
    "                    for s_1 in range(S):\n",
    "                        opt_prob += z[(h,s,a,s_1)] - (P_hat[s][a][s_1] + beta_prob[s][a,s_1]) *  pulp.lpSum([z[(h,s,a,y)] for y in range(S)]) <= 0\n",
    "                        opt_prob += -z[(h,s,a,s_1)] + (P_hat[s][a][s_1] - beta_prob[s][a,s_1])* pulp.lpSum([z[(h,s,a,y)] for y in range(S)]) <= 0\n",
    "\n",
    "    #問題を解く\n",
    "    status = problem.solve(pulp.PULP_CBC_CMD(msg=0,fracGap=0.0001))\n",
    "\n",
    "    if pulp.LpStatus[status] != 'Optimal':\n",
    "        print('Not Optimal')\n",
    "        return np.zeros((S,EPISODE_LENGTH,S)),np.zeros((S,EPISODE_LENGTH)),np.zeros((S, EPISODE_LENGTH)), pulp.LpStatus[status], np.zeros((S, EPISODE_LENGTH,S))\n",
    "    \n",
    "    for h in range(EPISODE_LENGTH):\n",
    "        for s in range(S):\n",
    "            for a in range(A):\n",
    "                for s_ in range(S):\n",
    "                    z_opt[h][s][a][s_] = z[(h,s,a,s_)].varValue\n",
    "\n",
    "    z_s_a = np.sum(z_opt,axis=3)\n",
    "    z_s = np.sum(z_opt,axis=(2,3))\n",
    "\n",
    "    for h in range(EPISODE_LENGTH):\n",
    "        for s in range(S):\n",
    "            sum_probs = 0\n",
    "            for a in range(A):\n",
    "                policy[s,h,a] = z_s_a[h][s][a] / z_s[h][s]\n",
    "                sum_probs += policy[s,h,a]\n",
    "            \n",
    "            if sum_probs.sum(axis=(0,1,2)) == 0:\n",
    "                for a in range(A):\n",
    "                    policy[s,h,a] = 1 / A\n",
    "            \n",
    "            else:\n",
    "                for a in range(A):\n",
    "                    policy[s,h,a] = policy[s,h,a] / sum_probs\n",
    "    \n",
    "    q_policy, value_of_policy, cost_of_policy = FiniteHorizon_Policy_evaluation(P, policy,R,C)\n",
    "\n",
    "    return policy,value_of_policy,cost_of_policy, pulp.LpStatus[status], q_policy\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syumi-note",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
