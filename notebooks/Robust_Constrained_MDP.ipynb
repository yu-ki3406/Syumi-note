{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Constrained MDP\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はこの[論文](https://openreview.net/pdf?id=e-ZdxsIwweR)を参考にConsttrained MDPとRobust MDPをうまく混合させることについてコードも含めて説明していきます．\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "from typing import Optional,NamedTuple\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_array:np.ndarray #状態空間\n",
    "    A_array:np.ndarray #行動空間\n",
    "    gamma:float #割引率\n",
    "    horizon:int #ホライゾン\n",
    "    rew:np.ndarray #報酬関数\n",
    "    con:np.ndarray #制約関数\n",
    "    P:np.ndarray #遷移行列\n",
    "    optimal_Q:Optional[np.ndarray] = None #最適なQ関数\n",
    "\n",
    "    @property\n",
    "    def S(self):\n",
    "        return len(self.S_array)\n",
    "\n",
    "    @property\n",
    "    def A(self):\n",
    "        return len(self.A_array)\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "S = 5 #状態数\n",
    "A = 3 #行動数\n",
    "rew = np.random.rand(S,A) #報酬関数\n",
    "con = np.zeros((S,A)) #制約関数\n",
    "\n",
    "con[0,2] = 1.0\n",
    "con[1,1] = 1.0\n",
    "con[1,2] = 1.0\n",
    "con[4,2] = 1.0\n",
    "\n",
    "P = np.random.rand(S*A,S) #遷移行列\n",
    "P = P / P.sum(axis=-1,keepdims=True)\n",
    "P = P.reshape(S, A, S)\n",
    "np.testing.assert_almost_equal(P.sum(axis=-1), 1)\n",
    "gamma = 0.9\n",
    "horizon = int(1/(1-gamma)) * 2\n",
    "S_array = np.arange(S)\n",
    "A_array = np.arange(A)\n",
    "\n",
    "mdp = MDP(S_array,A_array,gamma,horizon,rew,con,P)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "@jax.jit\n",
    "def compute_greedy_policy(Q:jnp.ndarray):\n",
    "    S,A = Q.shape\n",
    "    greedy_policy = jnp.zeros((S,A))\n",
    "    greedy_policy = greedy_policy.at[jnp.arange(S),Q.argmax(axis=-1)].set(1.0)\n",
    "    return greedy_policy\n",
    "\n",
    "@partial(jax.jit,static_argnames=('S','A'))\n",
    "def _compute_optimal_Q(mdp:MDP,S:int,A:int):\n",
    "\n",
    "    def backup(optimal_Q):\n",
    "        optimal_Q = optimal_Q.max(axis=-1)\n",
    "        return mdp.rew + mdp.gamma * mdp.P @ optimal_Q\n",
    "    \n",
    "    optimal_Q = jnp.zeros((S,A))\n",
    "    body_fn = lambda i,Q: backup(Q)\n",
    "    optimal_Q = jax.lax.fori_loop(0,mdp.horizon,body_fn,optimal_Q)\n",
    "    return optimal_Q\n",
    "\n",
    "compute_optimal_Q = lambda mdp : _compute_optimal_Q(mdp,mdp.S,mdp.A)\n",
    "\n",
    "\n",
    "@partial(jax.jit,static_argnames=('S','A'))\n",
    "def _compute_optimal_robust_Q(mdp:MDP,S:int,A:int):\n",
    "\n",
    "    def backup(optimal_Q):\n",
    "        policy = compute_greedy_policy(optimal_Q)\n",
    "        pi_Q = (policy * optimal_Q)\n",
    "        P = mdp.P.reshape(S*A,S)\n",
    "        P = P.min(axis=-1)\n",
    "        P = P.reshape(S,A)\n",
    "        return mdp.rew + mdp.gamma * P * pi_Q\n",
    "    \n",
    "    optimal_Q = jnp.zeros((S,A))\n",
    "    body_fn = lambda i,Q: backup(Q)\n",
    "    optimal_Q = jax.lax.fori_loop(0,mdp.horizon,body_fn,optimal_Q)\n",
    "    return optimal_Q\n",
    "\n",
    "\n",
    "\n",
    "compute_optimal_robust_Q = lambda mdp : _compute_optimal_robust_Q(mdp,mdp.S,mdp.A)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_greedy_constrained_policy(Q:jnp.ndarray):\n",
    "    S,A = Q.shape\n",
    "    greedy_policy = jnp.zeros((S,A))\n",
    "    Q_con = Q\n",
    "    # Q_con = Q_con.at[[0,2],[1,1],[4,2]].set(0)\n",
    "    Q_con = Q_con.at[0,2].set(0)\n",
    "    Q_con = Q_con.at[1,1].set(0)\n",
    "    Q_con = Q_con.at[1,2].set(0)\n",
    "    Q_con = Q_con.at[4,2].set(0)\n",
    "    \n",
    "    greedy_policy = greedy_policy.at[jnp.arange(S),Q_con.argmax(axis=-1)].set(1.0)\n",
    "    return greedy_policy\n",
    "\n",
    "@partial(jax.jit,static_argnames=('S','A'))\n",
    "def _compute_optimal_robust_constrained_Q(mdp:MDP,S:int,A:int):\n",
    "\n",
    "    def backup(optimal_Q):\n",
    "        policy = compute_greedy_constrained_policy(optimal_Q)\n",
    "        pi_Q = (policy * optimal_Q)\n",
    "        P = mdp.P.reshape(S*A,S)\n",
    "        P = P.min(axis=-1)\n",
    "        P = P.reshape(S,A)\n",
    "        return mdp.rew + mdp.gamma * P * pi_Q\n",
    "    \n",
    "    optimal_Q = jnp.zeros((S,A))\n",
    "    body_fn = lambda i,Q: backup(Q)\n",
    "    optimal_Q = jax.lax.fori_loop(0,mdp.horizon,body_fn,optimal_Q)\n",
    "    return optimal_Q\n",
    "\n",
    "compute_optimal_robust_constrained_Q = lambda mdp : _compute_optimal_robust_constrained_Q(mdp,mdp.S,mdp.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適なQ値とロバストQ値の差分:6.196681499481201\n",
      "最適なrobust Q値とロバスト制約付きQ値の差分:0.01615595817565918\n"
     ]
    }
   ],
   "source": [
    "optimal_Q = compute_optimal_Q(mdp)\n",
    "optimal_robust_Q = compute_optimal_robust_Q(mdp)\n",
    "optimal_robust_constrained_Q = compute_optimal_robust_constrained_Q(mdp)\n",
    "\n",
    "print(f'最適なQ値とロバストQ値の差分:{jnp.abs(optimal_Q - optimal_robust_Q).max()}')\n",
    "print(f'最適なrobust Q値とロバスト制約付きQ値の差分:{jnp.abs(optimal_robust_Q - optimal_robust_constrained_Q).max()}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これ上手くいってんの？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5488135  0.7403013  0.60276335]\n",
      " [0.54934484 0.4236548  0.6458941 ]\n",
      " [0.4375872  0.891773   1.0493212 ]\n",
      " [0.3834415  0.8238071  0.5288949 ]\n",
      " [0.56804454 0.95234555 0.07103606]]\n",
      "[[0.5488135  0.7403013  0.60276335]\n",
      " [0.5448832  0.4236548  0.66205007]\n",
      " [0.4375872  0.891773   1.0493212 ]\n",
      " [0.3834415  0.8238071  0.5288949 ]\n",
      " [0.56804454 0.95234555 0.07103606]]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_robust_constrained_Q)\n",
    "print(optimal_robust_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syumi-note",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
