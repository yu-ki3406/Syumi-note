{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thompson sampling**(TS)によるバンディットのアルゴリズムを考えていきます．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回の実験は報酬は確率的に作られていく**確率バンディット**を考えていきます.\n",
    "\n",
    "**確率バンディット**とはアームiを引いた時に発生される報酬 $X_{i}$ は $P_{i}$ の分布に従って生成されるというものです．\n",
    "\n",
    "TSでは報酬の期待値 $\\mu_{i}$ は何らかのモデルの事前分布 $\\pi_{i}(\\mu_{i})$ から生成されていると仮定します．\n",
    "\n",
    "つまり, **報酬の期待値の情報が含まれている事前分布がうまく表現できていれば, 報酬の分布との積分をとり事後分布でそのアームが良いものなのかを確認できるということです．**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実験設定を説明していきます．\n",
    "$\\pi_{i}(\\mu_{i})$ は**ベータ分布**に従うと仮定します.\n",
    "\n",
    "ベータ分布は,\n",
    "$$f(X; \\alpha, \\beta) = \\frac{X^{\\alpha-1}(1-X)^{\\beta-1}}{B(\\alpha, \\beta)}$$\n",
    "\n",
    "$X$: 確率変数, \n",
    "$\\alpha$: パラメータ, \n",
    "$\\beta$: パラメータ, \n",
    "\n",
    "$B(\\alpha, \\beta)$ ベータ関数：\n",
    "$$B(\\alpha, \\beta) = \\int_0^1 X^{\\alpha-1}(1-X)^{\\beta-1} dx$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "報酬はベータ分布の共役分布である**ベルヌーイ分布**を想定します．\n",
    "$$P(X=k) = \\binom{n}{k}p^k(1-p)^{n-k}$$\n",
    "\n",
    "$X$: 確率変数, \n",
    "$k$: 結果が1になる回数, \n",
    "$n$: 試行回数, \n",
    "$p$: 成功確率\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回数tまでの観測された報酬の情報を$H_{t} = {X_{i(s)}}_{s=1}^{t-1}$ とします．\n",
    "\n",
    "これらの表記を使い, **期待値のモデルの事後分布**を考えます．\n",
    "\n",
    "t回のうちアームiを$n_{i} = N_{i}(t)$回引いたとし, 報酬1が$m$回, 報酬0が$n_{i}-m$が発生したとすると, **モデル$\\pi_{i}(\\mu_{i})$の事後分布**は次のようになります.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi_{i}(\\mu_{i}|H_{t}) = Beta(\\alpha+m, \\beta+n_{i}-m)$$\n",
    "\n",
    "となります．\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そして, バンディットのアルゴリズムを思い出すと, 1番報酬が高そうなアームを引くことがゴールでした．\n",
    "ここでアーム$i$が最善であることを仮定すると上の式はこのように書けます．\n",
    "$$\n",
    "\\pi(\\mu_{i}|H(t)) = \\int_{0}^{1}\\pi_{i}(x_{i}|H(t))(\\prod_{j\\ne i}\\int_{0}^{x_{i}}\\pi_{j}(x_{j}|H(t))dx_{j})dx_{i}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の式の直感的な意味としては, t回が終わった時に**アーム$i$の期待値はそれ以外のアームよりも高くなっていて欲しい**という意味です"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでなぜ積分範囲が[0,1]かというと, 今回の報酬には**ベルヌーイ分布を想定しており, ベルヌーイ分布の期待値は[0,1]に収まる**ためです."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかしながら, 上の計算は**一般的に困難**とされています.\n",
    "よって次の代替案を適用することにします.\n",
    "\n",
    "1) $\\hat\\mu_{i}$ を $\\pi(\\mu_{i}|H(t))$ に従って生成します．\n",
    "2) $\\hat\\mu_{i}$ を最大にするアーム$i$を引きます．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### ここからアルゴリズムの説明にはいっていきます．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Bandit:\n",
    "    def __init__(self, n_bandits, epsilon):\n",
    "        self.n_bandits = n_bandits\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.zeros(n_bandits)\n",
    "        self.N = np.zeros(n_bandits)\n",
    "        self.bandits = np.random.normal(0, 1, (n_bandits,))\n",
    "        \n",
    "    def choose_action(self):\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.randint(0, self.n_bandits - 1)\n",
    "        else:\n",
    "            action = np.argmax(self.Q)\n",
    "        return action\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        self.N[action] += 1\n",
    "        self.Q[action] += (reward - self.Q[action]) / self.N[action]\n",
    "        \n",
    "    def simulate(self, n_steps):\n",
    "        rewards = np.zeros(n_steps)\n",
    "        for i in range(n_steps):\n",
    "            action = self.choose_action()\n",
    "            reward = np.random.binomial(1, self.bandits[action])\n",
    "            rewards[i] = reward\n",
    "            self.update(action, reward)\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#今回はアームが4つの場合を想定します，\n",
    "class TS_bandit:\n",
    "    def __init__(self,alpha: float,beta: float):\n",
    "        '''\n",
    "        p -> それぞれ報酬が発生する確率\n",
    "        alpha,beta -> ベータ分布の確率\n",
    "        m -> それぞれの報酬が出た回数\n",
    "        n -> ぞれぞれが試行された数\n",
    "        '''\n",
    "        self.p = [0.1, 0.3, 0.5, 0.8] #最適なアームのインデックスは3\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.m = np.zeros(len(self.p))\n",
    "        self.n = np.zeros(len(self.p))\n",
    "\n",
    "\n",
    "    def choose_action(self,m: int,n: int):\n",
    "        \n",
    "        best_index = None\n",
    "        prev_sample = 0\n",
    "        for i in range(len(self.p)):\n",
    "            sample = np.random.beta(self.alpha + m[i], self.beta + n[i] - m[i], 1)\n",
    "            if best_index == None:\n",
    "                best_index = i\n",
    "                prev_sample = sample\n",
    "                continue\n",
    "\n",
    "            if sample > prev_sample:\n",
    "                best_index = i\n",
    "                prev_sample = sample\n",
    "\n",
    "        return best_index\n",
    "\n",
    "    def reward(self,index: int):\n",
    "        return np.random.binomial(1,self.p[index],1)\n",
    "\n",
    "    \n",
    "    def update(self,index,reward):\n",
    "        self.m[index] += reward\n",
    "        self.n[index] += 1\n",
    "\n",
    "    def simulate(self,N):\n",
    "        for i in range(N):\n",
    "            arm = self.choose_action(self.m,self.n)\n",
    "            reward = self.reward(arm)\n",
    "            self.update(arm,reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0が選ばれた回数:6.0回, 1が選ばれた回数:4.0回, 2が選ばれた回数:7.0回, 3が選ばれた回数:983.0回\n"
     ]
    }
   ],
   "source": [
    "ban = TS_bandit(1,1) #今回は一様分布で実験\n",
    "ban.simulate(1000)\n",
    "\n",
    "print(f'0が選ばれた回数:{ban.n[0]}回, 1が選ばれた回数:{ban.n[1]}回, 2が選ばれた回数:{ban.n[2]}回, 3が選ばれた回数:{ban.n[3]}回')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の結果を見ればわかるように最適なアーム3が1番惹かれていることがわかりますね．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syumi-note",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf02f90440595f5968a48e039387c923c9e6f6584bb1d7920e7063fafb8317aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
