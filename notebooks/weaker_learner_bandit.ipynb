{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 弱学習器を使ったバンディットアルゴリズム"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず，今回はバンディットのアリゴリズムにUCBを使います．\n",
    "\n",
    "UCBの詳しい解説はこちらの[ノートブック](https://github.com/yu-ki3406/Syumi-note/blob/main/notebooks/UCB.ipynb)に書いてあります．\n",
    "\n",
    "まず簡潔に，UCBの定義をすると，次のような式になります．\n",
    "\n",
    "$$\n",
    "\\bar{u}_i(t)=\\hat{u}_i(t)+\\sqrt{\\frac{\\log t}{N_i(t)}}\n",
    "$$\n",
    "\n",
    "$t$ は今までに試行した回数で，$N_i(t)$はt回までにiが引かれた回数,$\\hat{u}_i(t)$はt回までのアームiの平均価値です．\n",
    "\n",
    "右の式で1番大きくなったアームiを引いていくという流れになっています．\n",
    "\n",
    "---\n",
    "\n",
    "今回の弱学習器を使ったアルゴリズムでは$\\hat{u}_i(t)$をなんらかの学習器に置き換えるという作業をします．\n",
    "\n",
    "弱学習器の簡潔な理解としては，ランダムなモデルよりも少しだけ精度が良くなるというもです．\n",
    "\n",
    "今回のアルゴリズムで次の考えは使わないのですが，弱学習器はアンサンブルという概念を使えるので，たくさんの弱学習器を使うことで，良い精度が出せることがわかっています．\n",
    "\n",
    "例としては，XGBoostやニューラルネットワークなどが挙げられます．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#今回はアームが4つの場合を想定します，\n",
    "class UCB_bandit:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        N -> armがそれぞれ選ばれた回数\n",
    "        rewards -> armのそれぞれの総報酬\n",
    "        '''\n",
    "        self.p = [0.1, 0.3, 0.5, 0.8] #最適なアームのインデックスは3\n",
    "        self.N = np.zeros(len(self.p))\n",
    "        self.rewards = np.zeros(len(self.p))\n",
    "        self.d = [0,1,2,3]\n",
    "        self.weights = np.ones(len(self.p))/len(self.p)\n",
    "        self.models = [None] * len(self.p)\n",
    "\n",
    "\n",
    "\n",
    "    def choose_action(self,t: int):\n",
    "        #1回も選ばれてないアームをなくす．\n",
    "        if t in [0,1,2,3]:\n",
    "            return t\n",
    "\n",
    "        rewards = np.zeros(len(self.p))\n",
    "        for i in range(len(self.p)):\n",
    "            # reward = self.rewards[i]/self.N[i] + 0.01 * np.sqrt(np.log(t) / (2*self.N[i]))\n",
    "            reward = self.models[i].predict(np.array(self.d[i]).reshape(-1,1)) + 0.01 * np.sqrt(np.log(t) / (2*self.N[i]))\n",
    "            rewards[i] = reward\n",
    "        max_index = np.where(rewards == rewards.max())\n",
    "        index = np.random.choice(max_index[0])\n",
    "\n",
    "    \n",
    "        return index\n",
    "            \n",
    "\n",
    "    def reward(self,index: int):\n",
    "\n",
    "        return np.random.binomial(1,self.p[index],1)\n",
    "\n",
    "    \n",
    "    def update(self,index,reward):\n",
    "        self.rewards[index] += reward\n",
    "        self.N[index] += 1\n",
    "        self.weights[index] = np.exp(1e-7*self.rewards[index]) * self.weights[index]\n",
    "        X = self.d[index]\n",
    "        y = self.rewards[index]/self.N[index]\n",
    "        X,y = np.array(X).reshape(-1,1),np.array(y).reshape(-1,1)\n",
    "        self.models[index] = self.weaker_learner(X,y,self.weights[index])\n",
    "        \n",
    "\n",
    "\n",
    "    def simulate(self,T):\n",
    "        for i in tqdm(range(T)):\n",
    "            arm = self.choose_action(i)\n",
    "            \n",
    "            reward = self.reward(arm)\n",
    "\n",
    "            self.update(arm,reward)\n",
    "\n",
    "    def weaker_learner(self,X,y,weights):\n",
    "        model = LinearRegression()\n",
    "        # print(X,y,weights)\n",
    "        model.fit(X,y,sample_weight=weights)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1170.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0が選ばれた回数:1.0回, 1が選ばれた回数:9.0回, 2が選ばれた回数:1.0回, 3が選ばれた回数:989.0回\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "agent = UCB_bandit()\n",
    "agent.simulate(1000)\n",
    "print(f'0が選ばれた回数:{agent.N[0]}回, 1が選ばれた回数:{agent.N[1]}回, 2が選ばれた回数:{agent.N[2]}回, 3が選ばれた回数:{agent.N[3]}回')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syumi-note",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
