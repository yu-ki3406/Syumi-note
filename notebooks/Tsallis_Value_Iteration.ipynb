{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tsallisエントロピーを用いた価値反復法\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はこの[論文](https://arxiv.org/pdf/1902.00137.pdf)についてコードも踏まえて解説していきたいと思います。\n",
    "\n",
    "Tsallisエントロピー(TE)はシャノンエントロピーなどを一般化したようなものです。\n",
    "\n",
    "次のq-logarithmを定義します。\n",
    "\n",
    "$$\n",
    "\\ln _q(x) \\triangleq \\begin{cases}\\log (x), & \\text { if } q=1 \\text { and } x>0 \\\\ \\frac{x^q-1}{q-1}, & \\text { if } q \\neq 1 \\text { and } x>0,\\end{cases}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この$ln_q(x)$はxに関して単調増加です。\n",
    "\n",
    "そしてその勾配は正です。\n",
    "\n",
    "次にTEの定義を示します。\n",
    "\n",
    "$$\n",
    "S_q(P) \\triangleq \\underset{X \\sim P}{\\mathbb{E}}\\left[-\\ln _q(P(X))\\right]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上のTEは$q>0$のときはconcave関数になり$q\\leq0$のときはconvex関数になります。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEを使った価値反復法は次のような作用素を適用します。\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "{\\left[\\mathcal{T}_q F\\right](s, a) \\triangleq \\underset{s^{\\prime} \\sim P}{\\mathbb{E}}\\left[\\mathbf{r}\\left(s, a, s^{\\prime}\\right)+\\gamma V_F(s) \\mid s, a\\right]} \\\\\n",
    "V_F(s) \\triangleq \\underset{a^{\\prime}}{q-\\max }\\left(F\\left(s, a^{\\prime}\\right)\\right) .\n",
    "\\end{gathered}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F$は任意の関数として考えているので、一般的な強化学習の場合の行動価値観数としてみなして大丈夫です。\n",
    "\n",
    "ここで、q-maxは次のような定義からなります。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "q-\\max _x(f(x)) \\triangleq \\max _{P \\in \\Delta}\\left[\\underset{X \\sim P}{\\mathbb{E}}[f(X)]+S_q(P)\\right],\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみにq-maxは$max_aQ(s,a)$を取るよりも値が大きくなることが証明されています。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEは累積報酬和の他にエントロピーも考慮した最大化を目指していいるので、TEで得られて最適方策は基のMDPでの最適方策とは違います。\n",
    "\n",
    "$\\pi^*$を基のMDPでの最適方策、$\\pi_q^*$をTEを考えたときの最適方策とすると、累積報酬和は次のような関係を持ちます。\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J\\left(\\pi^{\\star}\\right)+(1-\\gamma)^{-1} \\ln _q(1 /|\\mathcal{A}|) \\leq J\\left(\\pi_q^{\\star}\\right) \\leq J\\left(\\pi^{\\star}\\right)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コードに入っていきましょう。\n",
    "\n",
    "qの値の違いでどれだけ価値観数が異なるのか確認していきます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "from functools import partial\n",
    "\n",
    "S = 5 # number of states\n",
    "A = 3 # number of actions\n",
    "S_array = np.arange(S)\n",
    "A_array = np.arange(A)\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "rew = np.random.rand(S, A) # reward function\n",
    "P = np.random.rand(S*A, S) # transition function\n",
    "P = P / P.sum(axis=-1, keepdims=True) # normalize\n",
    "P = P.reshape(S,A,S)\n",
    "\n",
    "#MDPの定義\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_array : np.ndarray\n",
    "    A_array : np.ndarray\n",
    "    gamma : float\n",
    "    rew : np.ndarray\n",
    "    P : np.ndarray\n",
    "\n",
    "    @property\n",
    "    def S(self):\n",
    "        return len(self.S_array)\n",
    "    \n",
    "    @property\n",
    "    def A(self):\n",
    "        return len(self.A_array)\n",
    "    \n",
    "mdp = MDP(S_array, A_array, gamma, rew, P)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q-logarithmの定義\n",
    "def q_logarithm(x,q):\n",
    "    if q == 1:\n",
    "        return np.log(x)\n",
    "    else:\n",
    "        return (x**(q-1)-1)/(q-1)\n",
    "    \n",
    "#Tsallis entropyの定義\n",
    "def Tsallis_entropy(x,q):\n",
    "    return -x * q_logarithm(x,q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "@jax.jit\n",
    "def value_iteration(mdp:MDP, v:jnp.ndarray):\n",
    "    S,A = mdp.S,mdp.A\n",
    "    v = v.max(axis = -1)\n",
    "    v_dash = mdp.rew + mdp.gamma * mdp.P @ v\n",
    "\n",
    "    max_action = jnp.zeros((S,A))\n",
    "    max_action = max_action.at[np.arange(S),v_dash.argmax(axis=1)].set(1)\n",
    "    return max_action * v_dash\n",
    "\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=('q'))\n",
    "def Tsallis_value_iteration(mdp:MDP, v:jnp.ndarray,q:float):\n",
    "    S,A = mdp.S,mdp.A\n",
    "    v = v.max(axis = -1)\n",
    "    v_dash = mdp.rew + mdp.gamma * mdp.P @ (v + Tsallis_entropy(1/A,q))\n",
    "    max_action = jnp.zeros((S,A))\n",
    "    max_action = max_action.at[np.arange(S),v_dash.argmax(axis=1)].set(1)\n",
    "    return max_action * v_dash\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 22/10000 [00:00<01:01, 161.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "S,A = mdp.S , mdp.A\n",
    "iteration = 10000\n",
    "errors = []\n",
    "epsilon = 0.1 #閾値\n",
    "v_before = jax.random.uniform(key,shape=[S,A])\n",
    "\n",
    "for i in tqdm(range(iteration)):\n",
    "    v_after = Tsallis_value_iteration(mdp,v_before,1.5)\n",
    "\n",
    "    error = np.abs(v_after-v_before).max()\n",
    "    #終了条件\n",
    "\n",
    "    if error < epsilon:\n",
    "        break\n",
    "\n",
    "    errors.append(error)\n",
    "    # v_afterとv_beforeの型は(5,3)になっているがmax_actionをとっているので，ある一列以外の値は0に抑えられている．\n",
    "    v_before = v_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.723679 0.       0.      ]\n",
      " [0.       9.612428 0.      ]\n",
      " [0.       9.349348 0.      ]\n",
      " [9.617257 0.       0.      ]\n",
      " [0.       9.597204 0.      ]]\n"
     ]
    }
   ],
   "source": [
    "print(v_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/10000 [00:00<00:46, 216.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "S,A = mdp.S , mdp.A\n",
    "iteration = 10000\n",
    "errors = []\n",
    "epsilon = 0.1 #閾値\n",
    "v_before = jax.random.uniform(key,shape=[S,A])\n",
    "\n",
    "for i in tqdm(range(iteration)):\n",
    "    v_after = value_iteration(mdp,v_before)\n",
    "\n",
    "    error = np.abs(v_after-v_before).max()\n",
    "    #終了条件\n",
    "\n",
    "    if error < epsilon:\n",
    "        break\n",
    "\n",
    "    errors.append(error)\n",
    "    # v_afterとv_beforeの型は(5,3)になっているがmax_actionをとっているので，ある一列以外の値は0に抑えられている．\n",
    "    v_before = v_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[7.176655 , 0.       , 0.       ],\n",
       "       [0.       , 7.0654054, 0.       ],\n",
       "       [0.       , 6.8023257, 0.       ],\n",
       "       [7.070234 , 0.       , 0.       ],\n",
       "       [0.       , 7.0501814, 0.       ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syumi-note",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
