{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ロバストMDPのモデルフリーのオンライン学習について\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "import jax\n",
    "from typing import NamedTuple,Optional\n",
    "\n",
    "key = PRNGKey(0)\n",
    "\n",
    "S = 10\n",
    "A = 3\n",
    "S_array = jnp.arange(S)\n",
    "A_array = jnp.arange(A)\n",
    "gamma = 0.95\n",
    "\n",
    "key,_ = jax.random.split(key)\n",
    "rew = jax.random.uniform(key,shape=(S,A))\n",
    "\n",
    "key,_ = jax.random.split(key)\n",
    "P = jax.random.uniform(key,shape=(S*A,S))\n",
    "P = P / jnp.sum(P,axis=1,keepdims=True)\n",
    "P = P.reshape(S,A,S)\n",
    "\n",
    "#初期状態分布\n",
    "key,_ = jax.random.split(key)\n",
    "mu = jax.random.uniform(key,shape=(S,))\n",
    "mu = mu/jnp.sum(mu)\n",
    "\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_array : jnp.ndarray\n",
    "    A_array : jnp.ndarray\n",
    "    gamma : float\n",
    "    H : int \n",
    "    rew : jnp.ndarray\n",
    "    P : jnp.ndarray\n",
    "    mu : jnp.ndarray\n",
    "    optimal_Q : Optional[jnp.ndarray] = None #最適なQ値\n",
    "\n",
    "    @property\n",
    "    def S(self):\n",
    "        return len(self.S_array)\n",
    "    \n",
    "    @property\n",
    "    def A(self):\n",
    "        return len(self.A_array)\n",
    "    \n",
    "\n",
    "\n",
    "H = int(1/(1-gamma))\n",
    "mdp = MDP(S_array,A_array,gamma,H,rew,P,mu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import chex\n",
    "\n",
    "@jax.jit\n",
    "def compute_greedy_policy(Q: jnp.ndarray) -> jnp.ndarray:\n",
    "    greedy_policy = jnp.zeros_like(Q)\n",
    "    S,A = Q.shape\n",
    "    greedy_policy = greedy_policy.at[jnp.arange(S),jnp.argmax(Q,axis=1)].set(1)\n",
    "    return greedy_policy\n",
    "\n",
    "@partial(jax.jit,static_argnames=('S','A'))\n",
    "def _compute_optimal_Q(mdp:MDP,S:int,A:int) -> jnp.ndarray:\n",
    "    optimal_Q = jnp.zeros((S,A))\n",
    "\n",
    "    def backup(Q):\n",
    "        next_v = mdp.P @ jnp.max(Q,axis=1)\n",
    "        return mdp.rew + mdp.gamma * next_v\n",
    "    \n",
    "    body_fn = lambda i,Q: backup(Q)\n",
    "    return jax.lax.fori_loop(0,mdp.H+100,body_fn,optimal_Q)\n",
    "\n",
    "compute_optimal_Q = lambda mdp: _compute_optimal_Q(mdp,mdp.S,mdp.A)\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_Q(mdp:MDP,policy:jnp.ndarray):\n",
    "    S,A = policy.shape\n",
    "    chex.assert_shape(policy,(S,A))\n",
    "\n",
    "    def backup(Q):\n",
    "        v = (policy * Q).sum(axis=1)\n",
    "        next_v = mdp.P @ v\n",
    "        return mdp.rew + mdp.gamma * next_v\n",
    "    \n",
    "    policy_Q = jnp.zeros_like(policy)\n",
    "    body_fn = lambda i, policy_Q: backup(policy_Q)\n",
    "    return jax.lax.fori_loop(0,mdp.H+100,body_fn,policy_Q)\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_matrix(policy:jnp.ndarray): #訪問頻度の確率求めたいから，遷移確率行列と内積取れるような形にする\n",
    "    S,A = policy.shape\n",
    "    PI = policy.reshape(1,S,A)\n",
    "    PI = jnp.tile(PI,(S,1,1))\n",
    "    eyes = jnp.eyes(S).reshape(S,S,1)\n",
    "    PI = PI * eyes\n",
    "    PI = PI.reshape(S,S*A)\n",
    "\n",
    "    return PI\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_visit_sa(mdp:MDP,policy:jnp.ndarray,init_dist:jnp.ndarray):\n",
    "    S,A = policy.shape\n",
    "    chex.assert_shape(policy,(S,A))\n",
    "    PI = compute_policy_matrix(policy)\n",
    "    PPI = mdp.P.reshape(S*A,S) @ PI\n",
    "\n",
    "    def backup(visit):\n",
    "        next_visit = mdp.gamma * visit @ PPI\n",
    "        return init_dist @ PI + next_visit\n",
    "    \n",
    "    body_fn = lambda i,visit: backup(visit)\n",
    "    visit = jnp.zeros(S*A)\n",
    "    visit = jax.lax.fori_loop(0,mdp.H+100,body_fn,visit)\n",
    "    visit = visit.reshape(S,A)\n",
    "\n",
    "    return visit\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_visit_s(mdp: MDP, policy: jnp.ndarray, init_dist: jnp.ndarray):\n",
    "    S, A = policy.shape\n",
    "    chex.assert_shape(policy, (mdp.S, mdp.A))\n",
    "    Pi = compute_policy_matrix(policy)\n",
    "    PiP = Pi @ mdp.P.reshape(S*A, S) \n",
    "\n",
    "    def backup(visit):\n",
    "        next_visit = mdp.gamma * visit @ PiP\n",
    "        return init_dist + next_visit\n",
    "    \n",
    "    body_fn = lambda i, visit: backup(visit)\n",
    "    visit = jnp.zeros(S)\n",
    "    visit = jax.lax.fori_loop(0, mdp.H + 100, body_fn, visit)\n",
    "    return visit\n",
    "\n",
    "\n",
    "optimal_Q_DP = compute_optimal_Q(mdp)\n",
    "mdp = mdp._replace(optimal_Q=optimal_Q_DP)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "普通のQ学習とrobustなQ学習との比較のために二つのアルゴリズムを実装\n",
    "\n",
    "**Q学習**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop回すまでにかかった時間は1.1108109951019287秒です\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(1.04904175e-05, dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "@partial(jax.jit,static_argnames=('T'))\n",
    "def Q_learning(mdp:MDP,T:int,key:PRNGKey,lr:float=0.1,epsilon:float=0.0):\n",
    "    S,A = mdp.S,mdp.A\n",
    "\n",
    "    def body_fn(n,args):\n",
    "        key,s,Q = args\n",
    "\n",
    "        #epsilon-greedy\n",
    "        a = Q[s].argmax()\n",
    "        key,key1,key2 = jax.random.split(key,3)\n",
    "        random_A = jax.random.choice(key1,A)\n",
    "        a = jnp.where(jax.random.uniform(key2) < epsilon,random_A,a)\n",
    "\n",
    "        #遷移\n",
    "        key,key1 = jax.random.split(key,2)\n",
    "        next_s = jax.random.choice(key1,mdp.S_array,p=mdp.P[s,a])\n",
    "\n",
    "        next_V = Q[next_s].max(axis = -1)\n",
    "        Q_targ = mdp.rew[s,a] + mdp.gamma * next_V\n",
    "        Q_targ = (1-lr) * Q[s,a] + lr * Q_targ\n",
    "        Q = Q.at[s,a].set(Q_targ)\n",
    "\n",
    "        return key,next_s,Q\n",
    "    \n",
    "    Q = jnp.zeros((S,A))\n",
    "    key,new_key = jax.random.split(key)\n",
    "    init_s = jax.random.choice(new_key,mdp.S_array,p=mdp.mu)\n",
    "    args = (key,init_s,Q)\n",
    "    key,_,Q = jax.lax.fori_loop(0,T,body_fn,args)\n",
    "    return key,Q\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "start_time = time.time()\n",
    "key, Q = Q_learning(mdp, 100000, key, lr=0.1, epsilon=0.3)\n",
    "end_time = time.time()\n",
    "\n",
    "greedy_policy = compute_greedy_policy(Q)\n",
    "error = mdp.optimal_Q - compute_policy_Q(mdp, greedy_policy)\n",
    "print(f'loop回すまでにかかった時間は{end_time - start_time}秒です')\n",
    "\n",
    "error.max()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ロバストQ習**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop回すまでにかかった時間は1.1249217987060547秒です\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(1.04904175e-05, dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@partial(jax.jit,static_argnames=('T'))\n",
    "def Robust_Q_learning(mdp:MDP,T:int,key:PRNGKey,R:float,lr:float=0.1,epsilon:float=0.0):\n",
    "    S,A = mdp.S,mdp.A\n",
    "\n",
    "    def body_fn(n,args):\n",
    "        key,s,Q = args\n",
    "\n",
    "        a = Q[s].argmax()\n",
    "        key,key1,key2 = jax.random.split(key,3)\n",
    "        random_a = jax.random.choice(key1,A)\n",
    "        a = jnp.where(jax.random.uniform(key2) < epsilon,random_a,a)\n",
    "\n",
    "        #遷移\n",
    "        key,key1 = jax.random.split(key,2)\n",
    "        next_s = jax.random.choice(key1,mdp.S_array,p=mdp.P[s,a])\n",
    "\n",
    "        next_V = Q[next_s].max(axis = -1)\n",
    "        worst_V = Q.max(axis=1).min()\n",
    "        Q_targ = mdp.rew[s,a] + mdp.gamma *  (1-R) * next_V + mdp.gamma * R * worst_V\n",
    "        Q_targ = (1-lr) * Q[s,a] + lr * Q_targ\n",
    "        Q = Q.at[s,a].set(Q_targ)\n",
    "\n",
    "        return key,next_s,Q\n",
    "    Q = jnp.zeros((S,A))\n",
    "    key,new_key = jax.random.split(key)\n",
    "    init_s = jax.random.choice(new_key,mdp.S_array,p=mdp.mu)\n",
    "    args = (key,init_s,Q)\n",
    "    key,_,Q = jax.lax.fori_loop(0,T,body_fn,args)\n",
    "    return key,Q\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "start_time = time.time()\n",
    "key, robust_Q = Robust_Q_learning(mdp, 100000, key, R=0.4, lr=0.1, epsilon=0.3)\n",
    "end_time = time.time()\n",
    "robust_greedy_policy = compute_greedy_policy(Q)\n",
    "error = mdp.optimal_Q - compute_policy_Q(mdp, greedy_policy)\n",
    "print(f'loop回すまでにかかった時間は{end_time - start_time}秒です')\n",
    "error.max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "実際のロバストな方策が手に入ったのかを確認するために，元のMDPの遷移確率に摂動を入れてどのくらいロバストな方策なのか，確認する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def perturb_mdp(key, mdp: MDP, R: float):\n",
    "    key, _ = jax.random.split(key)\n",
    "    P = jax.random.uniform(key=key, shape=(S*A, S))\n",
    "    P = P / jnp.sum(P, axis=-1, keepdims=True)  # 正規化して確率にします\n",
    "    P = P.reshape(S, A, S)\n",
    "    perturbed_mdp = mdp._replace(P=(1 - R) * mdp.P + R * P)\n",
    "\n",
    "    optimal_Q_DP = compute_optimal_Q(perturbed_mdp)\n",
    "    perturbed_mdp = perturbed_mdp._replace(optimal_Q=optimal_Q_DP)\n",
    "    return key, perturbed_mdp\n",
    "\n",
    "key, perturbed_mdp = perturb_mdp(key, mdp, 0.7)\n",
    "np.testing.assert_allclose(perturbed_mdp.P.sum(axis=-1), 1, atol=1e-6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "摂動を入れたMDPでのQ学習の誤差は0.10248088836669922です\n"
     ]
    }
   ],
   "source": [
    "\n",
    "greedy_policy = compute_greedy_policy(Q)\n",
    "error = perturbed_mdp.optimal_Q - compute_policy_Q(perturbed_mdp, greedy_policy)\n",
    "print(f'摂動を入れたMDPでのQ学習の誤差は{error.max()}です')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "摂動を入れたMDPでのロバストQ学習の誤差は0.0です\n"
     ]
    }
   ],
   "source": [
    "greedy_policy = compute_greedy_policy(robust_Q)\n",
    "error = perturbed_mdp.optimal_Q - compute_policy_Q(perturbed_mdp, greedy_policy)\n",
    "print(f'摂動を入れたMDPでのロバストQ学習の誤差は{error.max()}です')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ロバストQ学習で学習された方策の方が違うMDPで試してもうまくいってそうで"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "す"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syumi-note",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
