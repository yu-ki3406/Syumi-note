{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **バンディットアルゴリズムの理論限界**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は私の勉強のためにもバンディットアルゴリズムの**理論限界**についてノートブックで説明していきたいと思います．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 理論限界\n",
    "\n",
    "バンディットアルゴリズムでは**最適な報酬**を得るために, 探索を必要とするのですが, ある一定の探索回数にに満たないときに, 最適なアームを選ぶ方針を導き出せないとされています．\n",
    "\n",
    "***例***: *アーム1は10%の確率で報酬**1**を出し, アーム2は1%の確率で報酬**100**を出すとします.この時, アームを2を引くのが最適な方針なのですが, アーム2から報酬を出すには平均で100回の試行が必要なため, **10回**などの探索で辞めてしまったら最適な方針を導けませんね.*\n",
    "\n",
    "\n",
    "つまり, **最適な方針をどのくらい最低限の試行**で導けるのかが**理論限界**の意味するところです."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ここでコードで例を見てみましょう. 今回は別のノートブックで説明したETCアルゴリズムで確認."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "p = np.array([0.1,0.01]) #各アームの報酬が出る確率\n",
    "r = np.array([1,100]) #各アームの報酬\n",
    "cul_reward = np.zeros(2)\n",
    "\n",
    "class Theory_bandit:\n",
    "    def __init__(self,t):\n",
    "       self.p = np.array([0.1,0.01]) #各アームの報酬が出る確率\n",
    "       self.r = np.array([1,100]) #各アームの報酬\n",
    "       self.cul_reward = np.zeros(2)  #累積報酬\n",
    "       self.choice_arm = np.zeros(2) #選択総回数\n",
    "       self.t = t #各アームの探索数\n",
    "    \n",
    "    def get_reward(self,index):\n",
    "        reward = self.r[index] if np.random.rand() < self.p[index] else 0\n",
    "        return reward\n",
    "\n",
    "    def update(self,index,reward):\n",
    "        self.cul_reward[index] += reward\n",
    "        self.choice_arm[index] += 1\n",
    "\n",
    "    def act(self,round):\n",
    "        if round < 2 * self.t: #決められた探索回数まで0か1を引く続ける\n",
    "            if round < self.t: \n",
    "                return 0\n",
    "\n",
    "            elif round < 2 * self.t:\n",
    "                return 1\n",
    "\n",
    "        else:\n",
    "            return np.argmax(self.cul_reward)\n",
    "\n",
    "    def simulate(self,T):\n",
    "        for round in range(T):\n",
    "            arm = self.act(round)\n",
    "            reward = self.get_reward(arm)\n",
    "            self.update(arm,reward)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の例で示したモデルがこのようなコードで書けました.\n",
    "\n",
    "\n",
    "次に各アームをそれぞれ10回, 100回探索した時の選択されたアームの回数を確認しましょう."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Theory_bandit(10)\n",
    "agent.simulate(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10回の探索でそれぞれのアームが試行された回数はアーム0が990.0回, アーム1が10.0回です.\n"
     ]
    }
   ],
   "source": [
    "print(f'10回の探索でそれぞれのアームが試行された回数はアーム0が{agent.choice_arm[0]}回, アーム1が{agent.choice_arm[1]}回です.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Theory_bandit(100)\n",
    "agent.simulate(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100回の探索でそれぞれのアームが試行された回数はアーム0が100.0, アーム1が900.0です.\n"
     ]
    }
   ],
   "source": [
    "print(f'100回の探索でそれぞれのアームが試行された回数はアーム0が{agent.choice_arm[0]}, アーム1が{agent.choice_arm[1]}です.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように10回の探索では最適な方針を導けないことがわかりましたね.\n",
    "\n",
    "単純に探索回数を減らすのではなく, 最適な方針を導ける且つ, 最低限の探索を目指すのがバンディットアルゴリズムということがわかります．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで最適な方針を導くための条件として**一貫性**という定理があります．\n",
    "\n",
    "### 一貫性\n",
    "\n",
    "*ある方策が一貫性を持つとは, 任意の固定した $a>0$ と真の確率分布の組 $({P_{i}})^K_{i=1} \\in P^k$ に対して, その方針のもとで $\\mathbb{E}[regret(T)] = \\mathrm{o}(T^a)$ が成り立つことを言う.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syumi-note",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf02f90440595f5968a48e039387c923c9e6f6584bb1d7920e7063fafb8317aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
